{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PGM-Lab/probai-2021-TAs/blob/main/Day2/notebooks/solutions_bayesian_regression_VI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfVmHy4Afyhm"
   },
   "source": [
    "<span style=\"color:red\">This notebook is an adapted version from  </span>  http://pyro.ai/examples/bayesian_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXcRh2TQfyhp"
   },
   "source": [
    "## Setup\n",
    "Let's begin by installing and importing the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EggRgZ1gfyhq"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ignore future warnings \n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtC9nacRfyhq"
   },
   "source": [
    "# Dataset \n",
    "\n",
    "The following example is taken from \\[1\\].  We would like to explore the relationship between topographic heterogeneity of a nation as measured by the Terrain Ruggedness Index (variable *rugged* in the dataset) and its GDP per capita. In particular, it was noted by the authors in \\[1\\] that terrain ruggedness or bad geography is related to poorer economic performance outside of Africa, but rugged terrains have had a reverse effect on income for African nations. Let us look at the data \\[2\\] and investigate this relationship.  We will be focusing on three features from the dataset:\n",
    "  - `rugged`: quantifies the Terrain Ruggedness Index\n",
    "  - `cont_africa`: whether the given nation is in Africa\n",
    "  - `rgdppc_2000`: Real GDP per capita for the year 2000\n",
    " \n",
    "  \n",
    "We will take the logarithm for the response variable GDP as it tends to vary exponentially. We also use a new variable `african_rugged`, defined as the product between the variables `rugged` and `cont_africa`, to capture the correlation between ruggedness and whether a country is in Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akcHul9xfyhr"
   },
   "outputs": [],
   "source": [
    "DATA_URL = \"../../Day1/notebooks/rugged_data.csv\"\n",
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n",
    "df[\"african_rugged\"] = data[\"cont_africa\"] * data[\"rugged\"]\n",
    "df = df[[\"cont_africa\", \"rugged\", \"african_rugged\", \"rgdppc_2000\"]]\n",
    "\n",
    "# Divide the data into predictors and response and store the data in numpy arrays\n",
    "data_array = np.array(df)\n",
    "x_data = data_array[:, :-1]\n",
    "y_data = data_array[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "EjOGg_Dafyhu",
    "outputId": "153795fa-4800-4d76-9b95-0c37bbde5e92"
   },
   "outputs": [],
   "source": [
    "# Display first 10 entries \n",
    "display(df[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFJeNXiUfyhx"
   },
   "source": [
    "# 1. Linear Regression\n",
    "\n",
    "Regression is one of the most common and basic supervised learning tasks in machine learning. Suppose we're given a dataset $\\mathcal{D}$ of the form\n",
    "\n",
    "$$ \\mathcal{D}  = \\{ (X_i, y_i) \\} \\qquad \\text{for}\\qquad i=1,2,...,N$$\n",
    "\n",
    "The goal of linear regression is to fit a function to the data of the form:\n",
    "\n",
    "$$ y = w X + b + \\epsilon $$\n",
    "\n",
    "where $w$ and $b$ are learnable parameters and $\\epsilon$ represents observation noise. Specifically $w$ is a matrix of weights and $b$ is a bias vector.\n",
    "\n",
    "Let's first implement linear regression in PyTorch and learn point estimates for the parameters $w$ and $b$.  Then we'll see how to incorporate uncertainty into our estimates by using Pyro to implement Bayesian regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "SD1O4RQ_fyhx",
    "outputId": "ce385a75-1d7a-40f7-82d0-7a813fb1a51c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "african_nations = data[data[\"cont_africa\"] == 1]\n",
    "non_african_nations = data[data[\"cont_africa\"] == 0]\n",
    "sns.scatterplot(non_african_nations[\"rugged\"], \n",
    "            np.log(non_african_nations[\"rgdppc_2000\"]), \n",
    "            ax=ax[0])\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "sns.scatterplot(african_nations[\"rugged\"], \n",
    "            np.log(african_nations[\"rgdppc_2000\"]), \n",
    "            ax=ax[1])\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHG9QQYhfyhy"
   },
   "source": [
    "## 1.1 Model\n",
    "We would like to predict log GDP per capita of a nation as a function of three features from the dataset - whether the nation is in Africa, its Terrain Ruggedness Index, and the interaction between these two.  Let's define our regression model. We'll define an specific object encapsulating this linear regression model.  Our input `x_data` is a tensor of size $N \\times 3$ and our output `y_data` is a tensor of size $N \\times 1$.  The method `predict(self,x_data)` defines a linear transformation of the form $Xw + b$ where $w$ is the weight matrix and $b$ is the additive bias.\n",
    "\n",
    "The parameters of the model are defined using ``torch.nn.Parameter``, and will be learned during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBQBgFkPfyhz"
   },
   "outputs": [],
   "source": [
    "class RegressionModel():\n",
    "    def __init__(self):\n",
    "        self.w = torch.nn.Parameter(torch.zeros(1, 3))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def params(self):\n",
    "        return {\"b\":self.b, \"w\": self.w}\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        return (self.b + torch.mm(self.w, torch.t(x_data))).squeeze(0)\n",
    "\n",
    "regression_model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlGU_7YPfyhz"
   },
   "source": [
    "## 1.2 Training\n",
    "We will use the mean squared error (MSE) as our loss and Adam as our optimizer. We would like to optimize the parameters of the `regression_model` neural net above. We will use a somewhat large learning rate of `0.05` and run for 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_N6WPDJufyh0",
    "outputId": "150fc76b-b3ab-48dc-ebb8-9ebfab049bcf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(regression_model.params().values(), lr=0.05)\n",
    "num_iterations = 5000\n",
    "data_array = torch.tensor(df.values, dtype=torch.float)\n",
    "x_data, y_data = data_array[:, :-1], data_array[:, -1]\n",
    "\n",
    "def main():\n",
    "    x_data = data_array[:, :-1]\n",
    "    y_data = data_array[:, -1]\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model.predict(x_data)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 500 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in regression_model.params().items():\n",
    "        print(name, param.data.numpy())\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyfAY0h-fyh0"
   },
   "source": [
    "## 1.3 Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmgazCZJfyh1"
   },
   "source": [
    "We now plot the regression line learned for african and non-african nations relating the rugeedness index with the GDP of the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "JWUs5dc1fyh1",
    "outputId": "76ef2e2e-69d9-4e77-d7ea-e5d292f723c4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Regression line \", fontsize=16)\n",
    "ax[0].scatter(x_data[x_data[:,0]==0,1].detach().numpy(), y_data[x_data[:,0]==0].detach().numpy())\n",
    "ax[1].scatter(x_data[x_data[:,0]==1,1].detach().numpy(), y_data[x_data[:,0]==1].detach().numpy())\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regression_model.predict(x_data[x_data[:,0]==0,:]).detach().numpy(), color='r')\n",
    "    ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regression_model.predict(x_data[x_data[:,0]==1,:]).detach().numpy(), color='r')\n",
    "\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"Non African Nations\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"African Nations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJw5o9kdfyh1"
   },
   "source": [
    "## 1.4 The relationship between ruggedness and log GPD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyWXJ1Tafyh2"
   },
   "source": [
    "Using this analysis, we can estimate the relationship between ruggedness and log GPD. As can be seen, this relationship is positive for African nations, but negative for Non African Nations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSB7vweCfyh2",
    "outputId": "9a147325-ea67-4773-9e7b-3f962f9035f4"
   },
   "outputs": [],
   "source": [
    "slope_within_africa = regression_model.params()['w'][0,1] + regression_model.params()['w'][0,2]\n",
    "slope_outside_africa = regression_model.params()['w'][0,1]\n",
    "print(slope_within_africa.detach().numpy())\n",
    "print(slope_outside_africa.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yrEaqT5fyh3"
   },
   "source": [
    "# 2. Bayesian Linear Regression\n",
    "\n",
    "\n",
    "[Bayesian modeling](http://mlg.eng.cam.ac.uk/zoubin/papers/NatureReprint15.pdf) offers a systematic framework for reasoning about model uncertainty. Instead of just learning point estimates, we're going to learn a _distribution_ over variables that is consistent with the observed data.\n",
    "\n",
    "In order to make our linear regression Bayesian, we need to put priors on the parameters $w$ and $b$. These are distributions that represent our prior belief about reasonable values for $w$ and $b$ (before observing any data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kltwl1J9fyh3"
   },
   "source": [
    "## 2.1 Model\n",
    "\n",
    "We now have all the ingredients needed to specify our model. First we define priors over weights and bias. Note the priors that we are using for the different latent variables in the model. \n",
    "\n",
    "The following figures shows a graphical description of the model: \n",
    "\n",
    "<img src=\"https://github.com/PGM-Lab/probai-2021-pyro/raw/main/Day2/Figures/BayesianLinearRegressionModel.png\" alt=\"Drawing\" width=800 >\n",
    "\n",
    "\n",
    "## 2.2 Full mean field\n",
    "First we consider a full mean filed approach, where the variational approximation factorizes as\n",
    "$$\n",
    "q({\\bf w}, b) = q(b)\\prod _{i=1}^Mq(w_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScKGP0kjF48e"
   },
   "source": [
    "### Helper-routine: Calculate ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mSXTEhsF34-"
   },
   "outputs": [],
   "source": [
    "def calculate_ELBO(x_data, y_data, gamma_w, gamma_b, theta, q_w_mean, q_w_prec, q_b_mean, q_b_prec):\n",
    "    \"\"\"\n",
    "    Helper routine: Calculate ELBO. Data is the sampled x and y values, gamma is the prior precision over the \n",
    "    weights and theta is the prior precision associated with y. Everything prefixed a 'q' relates to the \n",
    "    variational posterior.\n",
    "    \n",
    "    Note: This function obviously only works for this particular model and is not a general solution.\n",
    "\n",
    "    :param x_data: The predictors\n",
    "    :param y_data: The response variable\n",
    "    :param gamma_w: prior precision for the weights\n",
    "    :param gamma_b: prior precision for the intercept\n",
    "    :param theta: prior precision for y\n",
    "    :param q_w_mean: VB posterior mean for the distribution of the weights w \n",
    "    :param q_w_prec: VB posterior precision for the distribution of the weights w \n",
    "    :param q_b_mean: VB posterior mean for the intercept b\n",
    "    :param q_b_prec: VB posterior precision for the intercept b\n",
    "    :return: the ELBO\n",
    "    \"\"\"\n",
    "    \n",
    "    # We calculate the ELBO as E_q log p(y,x,w,b) - E_q log q(w,b), where\n",
    "    # log p(y,x,w) = sum_i log p(y|x,w,b) + log p(w) + log p(b)\n",
    "    # log q(w,b) = log q(w) + log q(b)\n",
    "\n",
    "    M = x_data.shape[1]\n",
    "\n",
    "    # E_q log p(w)\n",
    "    E_log_p = -0.5 * M * np.log(2 * np.pi) + 0.5 * M * gamma_w - 0.5 * gamma_w * np.sum(np.diagonal(np.linalg.inv(q_w_prec))\n",
    "                                                                                    + (q_w_mean*q_w_mean).flatten())\n",
    "    # E_q log p(b)\n",
    "    E_log_p += -0.5 * np.log(2 * np.pi) + 0.5 * np.log(gamma_b) - 0.5 * gamma_b * (1/q_b_prec + q_b_mean**2)\n",
    "\n",
    "    # sum_i E_q log p(y|x,w,b)\n",
    "    E_w_w = np.linalg.inv(q_w_prec) + q_w_mean @ q_w_mean.transpose()\n",
    "    E_b_b = 1/q_b_prec + q_b_mean**2\n",
    "    for i in range(x_data.shape[0]):\n",
    "        E_x_ww_x = np.matmul(x_data[i, :].transpose(), np.matmul(E_w_w, x_data[i, :]))\n",
    "        E_log_p += -0.5 * np.log(2 * np.pi) + 0.5 * np.log(theta) \\\n",
    "                   - 0.5 * theta * (y_data[i]**2 + E_x_ww_x + E_b_b\n",
    "                                    + 2 * q_b_mean * np.matmul(q_w_mean.transpose(), x_data[i, :])\n",
    "                                    - 2 * y_data[i] * np.matmul(q_w_mean.transpose(), x_data[i,:])\n",
    "                                    - 2 * y_data[i] * q_b_mean)\n",
    "\n",
    "    # Entropy of q_b\n",
    "    ent = 0.5 * np.log(1 * np.pi * np.exp(1) / q_b_prec)\n",
    "    ent += 0.5 * np.log(np.linalg.det(2 * np.pi * np.exp(1) * np.linalg.inv(q_w_prec)))\n",
    "\n",
    "    return E_log_p - ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUWfkBXUfyh3"
   },
   "source": [
    "### <span style=\"color:red\">Exercise 1: Introduce the variational updating rules</span> \n",
    "\n",
    "\n",
    "* Variational updating rules for $q(w_i)$ (already implemented below).\n",
    "  - Updating equation for **precision** of $q(w_i)$: \n",
    "$$\n",
    "\\tau \\leftarrow (\\gamma_w+\\theta\\sum_{i=1}^N(x_{ij}^2))\n",
    "$$\n",
    "  - Updating equation for **mean** of $q(w_i)$: \n",
    "$$\\mu \\leftarrow \\tau^{-1}\\theta\\sum_{i=1}^Nx_{ij}(y_i - (\\sum_{k\\neq j}x_{ik}\\mathbb{E}(W_k)+\\mathbb{E}(B)))\n",
    "$$\n",
    "\n",
    "* Introduce variational updating rules for $q(b)$, which is normally distributed.\n",
    "  - Updating equation for **precision** of $q(w_i)$: \n",
    "$$\n",
    "\\tau \\leftarrow (\\gamma_b+\\theta N)\n",
    "$$\n",
    "  - Updating equation for **mean** of $q(w_i)$: \n",
    "$$\n",
    "\\mu \\leftarrow \\tau^{-1} \\theta\\sum_{i=1}^N(y_i -\n",
    "              \\mathbb{E}(\\mathbf{W}^T)\\mathbf{x}_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_19buBJsfyh4"
   },
   "outputs": [],
   "source": [
    "# The variational updating rule for weight component 'comp'\n",
    "def update_w_comp(x_data, y_data, gamma_w, theta, q_w_mean, q_w_prec, q_b_mean, comp):\n",
    "\n",
    "    # Lenght of weight vector\n",
    "    M = x_data.shape[1]\n",
    "    # The precision (a scalar)\n",
    "    Q = gamma_w\n",
    "    # The mean (a scalar)\n",
    "    mu = 0.0\n",
    "    for i in range(x_data.shape[0]):\n",
    "        Q += theta * x_data[i, comp]**2\n",
    "        mu += (y_data[i] - q_b_mean - (np.sum(x_data[i, :] @ q_w_mean) - x_data[i, comp]*q_w_mean[comp])) \\\n",
    "                * x_data[i, comp]\n",
    "    mu = theta * 1/Q * mu\n",
    "\n",
    "    # Update the appropriate entries in the mean vector and precision matrix\n",
    "    q_w_prec[comp, comp] = Q\n",
    "    q_w_mean[comp] = mu.item()\n",
    "\n",
    "    return q_w_prec, q_w_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to code the variational updating rule for the intercetp $B$. This updating rule only depends on $\\textbf{x}$, $\\textbf{y}$, $\\gamma_b$, $\\theta$ and the mean of the variational posterior distribution over the weights $\\textbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variational updating rule for the intercept\n",
    "def update_b(x_data, y_data, gamma_b, theta, q_w_mean):\n",
    "\n",
    "    # The precision (a scalar)\n",
    "    tau = ???????\n",
    "    # The mean (a scalar)\n",
    "    mu = ???????\n",
    "\n",
    "    return tau, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktwd6CCUfyh8"
   },
   "source": [
    "## 2.3 Inference\n",
    "\n",
    "To do inference we'll use coordinate ascent, which is implemented by the above updating rules. Just like in the non-Bayesian linear regression, each iteration of our training objective will be optimzed, with the difference that in this case, we'll use the ELBO objective instead of the MSE loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4Wh1Tyqjfyh9",
    "outputId": "1d12ac96-9062-4965-b62d-07838712a095"
   },
   "outputs": [],
   "source": [
    "# Initialize the variational distributions\n",
    "data_array = np.array(df)\n",
    "x_data = data_array[:, :-1]\n",
    "y_data = data_array[:, -1]\n",
    "M = x_data.shape[1]\n",
    "gamma_w = 1\n",
    "gamma_b = 1\n",
    "theta = 1\n",
    "q_w_mean = np.random.normal(0, 1, (3, 1))\n",
    "q_w_prec = np.diag((1, 1, 1))\n",
    "q_b_mean = np.random.normal(0, 1)\n",
    "q_b_prec = 1\n",
    "elbos = []\n",
    "\n",
    "# Calculate ELBO\n",
    "this_lb = calculate_ELBO(x_data, y_data, gamma_w, gamma_b, theta, q_w_mean, q_w_prec, q_b_mean, q_b_prec)\n",
    "elbos.append(this_lb)\n",
    "previous_lb = -np.inf\n",
    "# Start iterating\n",
    "print(\"\\n\" + 100 * \"=\" + \"\\n   VB iterations:\\n\" + 100 * \"=\")\n",
    "for iteration in range(100):\n",
    "\n",
    "    # Update the variational distributions\n",
    "    for i in range(M):\n",
    "        q_w_prec, q_w_mean = update_w_comp(x_data, y_data, gamma_w, theta, q_w_mean, q_w_prec, q_b_mean, i)\n",
    "    q_b_prec, q_b_mean = update_b(x_data, y_data, gamma_b, theta, q_w_mean)\n",
    "\n",
    "    this_lb = calculate_ELBO(x_data, y_data, gamma_w, gamma_b, theta, q_w_mean, q_w_prec, q_b_mean, q_b_prec)\n",
    "    elbos.append(this_lb)\n",
    "    print(f\"Iteration {iteration:2d}. ELBO: {this_lb.item():13.7f}\")\n",
    "    if this_lb < previous_lb:\n",
    "        raise ValueError(\"ELBO is decreasing. Something is wrong! Goodbye...\")\n",
    "    \n",
    "    if iteration > 0 and np.abs((this_lb - previous_lb) / previous_lb) < 1E-8:\n",
    "        # Very little improvement. We are done.\n",
    "        break\n",
    "    \n",
    "    # If we didn't break we need to run again. Update the value for \"previous\"\n",
    "    previous_lb = this_lb\n",
    "print(\"\\n\" + 100 * \"=\" + \"\\n\")\n",
    "\n",
    "# Store the results\n",
    "w_mean_mf = q_w_mean\n",
    "w_prec_mf = q_w_prec\n",
    "b_mean_mf = q_b_mean\n",
    "b_prec_mf = q_b_prec\n",
    "\n",
    "plt.plot(range(len(elbos)), elbos)\n",
    "plt.xlabel('NUmber of iterations')\n",
    "plt.ylabel('ELBO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loTyP7zIMh2y"
   },
   "source": [
    "Now, we have a Gaussian posterior for $q(b)$ and $q(w)$ with means and precisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfaPyhfTfyh9",
    "outputId": "3a0d8f06-6a7c-4b7d-b9c9-cd8b8538268c"
   },
   "outputs": [],
   "source": [
    "print(\"Mean q(b):\", b_mean_mf)\n",
    "print(\"Precision q(b):\", b_prec_mf)\n",
    "print(\"Mean q(w):\", w_mean_mf)\n",
    "print(\"Precision q(w):\", w_prec_mf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKfYcpnYfyh-"
   },
   "source": [
    "Note that instead of just point estimates, we now have uncertainty estimates for our learned parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXTAFdgHfyiB"
   },
   "source": [
    "## 2.4  Model's Uncertainty\n",
    "We can now sample different regression lines from the variational posteriors, thus reflecting the model uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "MFD9r6XRfyiB",
    "outputId": "e7115978-a8d8-4c6e-ff20-c7ad3c51884c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Uncertainty in Regression line \", fontsize=16)\n",
    "num_samples = 20\n",
    "\n",
    "ax[0].scatter(x_data[x_data[:,0]==0,1], y_data[x_data[:,0]==0])\n",
    "for _ in range(num_samples):\n",
    "    b_sample = np.random.normal(loc=q_b_mean, scale=1/np.sqrt(q_b_prec))\n",
    "    w_sample = np.random.multivariate_normal(mean=q_w_mean.flatten(), cov=np.linalg.inv(q_w_prec))\n",
    "    ax[0].plot(x_data[x_data[:,0]==0,1], (x_data[x_data[:,0]==0,:] @ w_sample)+b_sample, 'r-')\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "\n",
    "ax[1].scatter(x_data[x_data[:,0]==1,1], y_data[x_data[:,0]==1])\n",
    "for _ in range(num_samples):\n",
    "    b_sample = np.random.normal(loc=q_b_mean, scale=1/np.sqrt(q_b_prec))\n",
    "    w_sample = np.random.multivariate_normal(mean=q_w_mean.flatten(), cov=np.linalg.inv(q_w_prec))\n",
    "    ax[1].plot(x_data[x_data[:,0]==1,1], (x_data[x_data[:,0]==1,:] @ w_sample)+b_sample, 'r-')\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQQTd-3KfyiD"
   },
   "source": [
    "The above figure shows the uncertainty in our estimate of the regression line. Note that for lower values of ruggedness there are many more data points, and as such, the regression lines are less uncertainty than in high ruggness values, where there is much more uncertainty, specially in the case of African nations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CptrzXtXfyiG"
   },
   "source": [
    "## 2.5 The relationship between ruggedness and log GPD\n",
    "\n",
    "Finally, we go back to the previous analysis about the relationship between ruggedness and log GPD. Now, we can compute uncertainties over this relationship. As can be seen, this relationship is negative for Non African Nations with high probability, and positive for African nations in most of the cases. But there is non-negligible probability that this is relationship is also negative. This is the consequence of the low number of samples in the case of African nations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "ov_5tYXsfyiH",
    "outputId": "b8b937a6-926b-467f-bbbe-39a5df09962a"
   },
   "outputs": [],
   "source": [
    "weight = np.random.multivariate_normal(mean=q_w_mean.flatten(), cov=np.linalg.inv(q_w_prec),size=1000)\n",
    "gamma_within_africa = weight[:,1] + weight[:,2]\n",
    "gamma_outside_africa = weight[:,1]\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.distplot(gamma_within_africa, kde_kws={\"label\": \"African nations\"},)\n",
    "sns.distplot(gamma_outside_africa, kde_kws={\"label\": \"Non-African nations\"})\n",
    "fig.suptitle(\"Density of Slope : log(GDP) vs. Terrain Ruggedness\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpo6kGPRfyiL"
   },
   "source": [
    "### References\n",
    "  1. McElreath, D., *Statistical Rethinking, Chapter 7*, 2016\n",
    "  2. Nunn, N. & Puga, D., *[Ruggedness: The blessing of bad geography in Africa\"](https://diegopuga.org/papers/rugged.pdf)*, Review of Economics and Statistics 94(1), Feb. 2012"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "include_colab_link": true,
   "name": "solutions_bayesian_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
